import os
import sys
import gc
import json
import psutil
import scanpy as sc
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import io
from scipy.sparse import csr_matrix
import anndata as ad
from typing import Union, Tuple, Optional
from tqdm import tqdm
import warnings
import hashlib
from datetime import datetime
import traceback

warnings.filterwarnings('ignore')

# è®¾ç½® matplotlib å‚æ•°
plt.rcParams['figure.dpi'] = 100
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 10


class CheckpointManager:
    """
    å¢å¼ºçš„æ–­ç‚¹ç®¡ç†å™¨ - æ”¯æŒä»ä¸­æ–­å¤„ç»§ç»­è¿è¡Œ

    æ–°å¢åŠŸèƒ½ï¼š
    - æ•°æ®æ–‡ä»¶å®Œæ•´æ€§æ ¡éªŒï¼ˆMD5ï¼‰
    - è‡ªåŠ¨æ£€æµ‹æŸåçš„æ–­ç‚¹
    - äº¤äº’å¼æ¢å¤ç¡®è®¤
    - è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
    """

    def __init__(self, output_dir: str, auto_resume: bool = False):
        self.output_dir = output_dir
        self.checkpoint_file = os.path.join(output_dir, '.checkpoint.json')
        self.error_log_file = os.path.join(output_dir, '.error_log.txt')
        self.auto_resume = auto_resume
        self.checkpoints = self.load_checkpoints()
        self.steps_order = ['step1_read', 'step2_qc', 'step3_integrate', 'step4_annotate']

    def load_checkpoints(self) -> dict:
        """åŠ è½½å·²æœ‰çš„æ£€æŸ¥ç‚¹å¹¶éªŒè¯å®Œæ•´æ€§"""
        if os.path.exists(self.checkpoint_file):
            try:
                with open(self.checkpoint_file, 'r') as f:
                    checkpoints = json.load(f)

                # éªŒè¯æ£€æŸ¥ç‚¹å®Œæ•´æ€§
                self._validate_checkpoints(checkpoints)
                return checkpoints
            except Exception as e:
                print(f"âš ï¸  åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {str(e)}")
                print("   å°†ä»å¤´å¼€å§‹è¿è¡Œ")
                return {}
        return {}

    def _validate_checkpoints(self, checkpoints: dict):
        """éªŒè¯æ£€æŸ¥ç‚¹æ•°æ®çš„å®Œæ•´æ€§"""
        for step_name, step_data in checkpoints.items():
            if not step_data.get('completed', False):
                continue

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            file_path = step_data.get('file')
            if file_path and not os.path.exists(file_path):
                print(f"âš ï¸  è­¦å‘Š: æ­¥éª¤ {step_name} çš„æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                step_data['valid'] = False
            else:
                step_data['valid'] = True

                # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§ï¼ˆå¯é€‰ï¼Œé’ˆå¯¹å¤§æ–‡ä»¶å¯èƒ½è¾ƒæ…¢ï¼‰
                if file_path and os.path.exists(file_path):
                    file_size = os.path.getsize(file_path)
                    saved_size = step_data.get('file_size')

                    if saved_size and file_size != saved_size:
                        print(f"âš ï¸  è­¦å‘Š: æ–‡ä»¶å¤§å°ä¸åŒ¹é… {os.path.basename(file_path)}")
                        print(f"   é¢„æœŸ: {saved_size}, å®é™…: {file_size}")
                        step_data['valid'] = False

    def save_checkpoint(self, step: str, **kwargs):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼‰

        è‡ªåŠ¨è®°å½•ï¼š
        - æ—¶é—´æˆ³
        - æ–‡ä»¶è·¯å¾„å’Œå¤§å°
        - å…³é”®ç»Ÿè®¡ä¿¡æ¯
        """
        checkpoint_data = {
            'completed': True,
            'timestamp': datetime.now().isoformat(),
            'valid': True,
            **kwargs
        }

        # å¦‚æœæœ‰æ–‡ä»¶è·¯å¾„ï¼Œè®°å½•æ–‡ä»¶å¤§å°
        if 'file' in kwargs and os.path.exists(kwargs['file']):
            checkpoint_data['file_size'] = os.path.getsize(kwargs['file'])

        self.checkpoints[step] = checkpoint_data

        try:
            with open(self.checkpoint_file, 'w') as f:
                json.dump(self.checkpoints, f, indent=2)
            print(f"   ğŸ’¾ æ–­ç‚¹å·²ä¿å­˜: {step}")
        except Exception as e:
            print(f"   âš ï¸  ä¿å­˜æ–­ç‚¹å¤±è´¥: {str(e)}")

    def mark_step_failed(self, step: str, error: Exception):
        """
        æ ‡è®°æ­¥éª¤å¤±è´¥å¹¶è®°å½•é”™è¯¯ä¿¡æ¯
        """
        error_info = {
            'failed': True,
            'timestamp': datetime.now().isoformat(),
            'error_type': type(error).__name__,
            'error_message': str(error),
            'traceback': traceback.format_exc()
        }

        self.checkpoints[step] = error_info

        # ä¿å­˜åˆ°ä¸»æ£€æŸ¥ç‚¹æ–‡ä»¶
        try:
            with open(self.checkpoint_file, 'w') as f:
                json.dump(self.checkpoints, f, indent=2)
        except:
            pass

        # è¿½åŠ åˆ°é”™è¯¯æ—¥å¿—
        try:
            with open(self.error_log_file, 'a') as f:
                f.write(f"\n{'='*80}\n")
                f.write(f"æ­¥éª¤: {step}\n")
                f.write(f"æ—¶é—´: {error_info['timestamp']}\n")
                f.write(f"é”™è¯¯ç±»å‹: {error_info['error_type']}\n")
                f.write(f"é”™è¯¯ä¿¡æ¯: {error_info['error_message']}\n")
                f.write(f"è¯¦ç»†è¿½è¸ª:\n{error_info['traceback']}\n")
        except:
            pass

        print(f"   âŒ æ­¥éª¤ {step} å¤±è´¥ï¼Œé”™è¯¯å·²è®°å½•")
        print(f"   é”™è¯¯æ—¥å¿—: {self.error_log_file}")

    def is_completed(self, step: str) -> bool:
        """æ£€æŸ¥æŸä¸ªæ­¥éª¤æ˜¯å¦å·²å®Œæˆä¸”æœ‰æ•ˆ"""
        step_data = self.checkpoints.get(step, {})
        return step_data.get('completed', False) and step_data.get('valid', True)

    def is_failed(self, step: str) -> bool:
        """æ£€æŸ¥æŸä¸ªæ­¥éª¤æ˜¯å¦å¤±è´¥è¿‡"""
        return self.checkpoints.get(step, {}).get('failed', False)

    def get_checkpoint_data(self, step: str) -> dict:
        """è·å–æ£€æŸ¥ç‚¹æ•°æ®"""
        return self.checkpoints.get(step, {})

    def reset(self, confirm: bool = False):
        """é‡ç½®æ‰€æœ‰æ£€æŸ¥ç‚¹ï¼ˆä»å¤´å¼€å§‹ï¼‰"""
        if not confirm and not self.auto_resume:
            response = input("âš ï¸  ç¡®å®šè¦é‡ç½®æ‰€æœ‰æ–­ç‚¹å—ï¼Ÿ(y/n): ")
            if response.lower() != 'y':
                print("å–æ¶ˆé‡ç½®")
                return

        self.checkpoints = {}
        if os.path.exists(self.checkpoint_file):
            os.remove(self.checkpoint_file)
        print("âœ… å·²é‡ç½®æ‰€æœ‰æ£€æŸ¥ç‚¹")

    def get_last_completed_step(self) -> Optional[str]:
        """è·å–æœ€åä¸€ä¸ªå®Œæˆçš„æ­¥éª¤"""
        for step in reversed(self.steps_order):
            if self.is_completed(step):
                return step
        return None

    def get_next_step(self) -> Optional[str]:
        """è·å–ä¸‹ä¸€ä¸ªéœ€è¦æ‰§è¡Œçš„æ­¥éª¤"""
        for step in self.steps_order:
            if not self.is_completed(step):
                return step
        return None

    def print_status(self):
        """æ‰“å°å½“å‰è¿›åº¦çŠ¶æ€ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        step_names = {
            'step1_read': 'æ­¥éª¤1: æ•°æ®è¯»å–',
            'step2_qc': 'æ­¥éª¤2: è´¨é‡æ§åˆ¶',
            'step3_integrate': 'æ­¥éª¤3: æ•°æ®æ•´åˆ',
            'step4_annotate': 'æ­¥éª¤4: ç»†èƒæ³¨é‡Š'
        }

        print("\n" + "=" * 80)
        print("å½“å‰åˆ†æè¿›åº¦")
        print("=" * 80)

        for step_id in self.steps_order:
            step_name = step_names.get(step_id, step_id)

            if self.is_failed(step_id):
                data = self.get_checkpoint_data(step_id)
                print(f"âŒ {step_name} - å¤±è´¥")
                print(f"   é”™è¯¯: {data.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
                print(f"   æ—¶é—´: {data.get('timestamp', '')[:19]}")

            elif self.is_completed(step_id):
                data = self.get_checkpoint_data(step_id)
                timestamp = data.get('timestamp', '')
                file_path = data.get('file', '')

                print(f"âœ… {step_name}")
                if timestamp:
                    print(f"   å®Œæˆæ—¶é—´: {timestamp[:19]}")
                if file_path and os.path.exists(file_path):
                    size_mb = os.path.getsize(file_path) / 1024**2
                    print(f"   æ–‡ä»¶: {os.path.basename(file_path)} ({size_mb:.1f} MB)")

                # æ˜¾ç¤ºå…³é”®ç»Ÿè®¡
                if 'n_cells' in data:
                    print(f"   ç»†èƒæ•°: {data['n_cells']:,}")
                if 'n_genes' in data:
                    print(f"   åŸºå› æ•°: {data['n_genes']:,}")
                if 'n_clusters' in data:
                    print(f"   èšç±»æ•°: {data['n_clusters']}")
            else:
                print(f"â³ {step_name} - å¾…æ‰§è¡Œ")

        print("=" * 80)

        # æ˜¾ç¤ºä¸‹ä¸€æ­¥æ“ä½œ
        next_step = self.get_next_step()
        if next_step:
            print(f"\nğŸ“ ä¸‹ä¸€æ­¥: {step_names.get(next_step, next_step)}")
        else:
            print(f"\nğŸ‰ æ‰€æœ‰æ­¥éª¤å·²å®Œæˆï¼")
        print()

    def should_resume(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä»æ–­ç‚¹æ¢å¤"""
        last_step = self.get_last_completed_step()

        if not last_step:
            return False

        if self.auto_resume:
            print(f"ğŸ”„ è‡ªåŠ¨ä»æ–­ç‚¹æ¢å¤: {last_step}")
            return True

        # äº¤äº’å¼ç¡®è®¤
        print(f"\næ£€æµ‹åˆ°ä¹‹å‰çš„è¿›åº¦:")
        self.print_status()

        response = input("æ˜¯å¦ä»ä¸Šæ¬¡æ–­ç‚¹ç»§ç»­è¿è¡Œï¼Ÿ(y/n): ")
        return response.lower() == 'y'


class MemoryMonitor:
    """å†…å­˜ç›‘æ§ç±»"""

    def __init__(self):
        self.process = psutil.Process()
        self.checkpoints = []

    def get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨ï¼ˆGBï¼‰"""
        return self.process.memory_info().rss / 1024**3

    def checkpoint(self, step_name: str):
        """è®°å½•æ£€æŸ¥ç‚¹"""
        mem_gb = self.get_memory_usage()
        self.checkpoints.append({'step': step_name, 'memory_gb': mem_gb})
        print(f"   ğŸ“Š å†…å­˜ä½¿ç”¨: {mem_gb:.2f} GB ({step_name})")

    def get_summary(self) -> pd.DataFrame:
        """è·å–å†…å­˜ä½¿ç”¨æ‘˜è¦"""
        df = pd.DataFrame(self.checkpoints)
        if len(df) > 0:
            df['memory_increase_gb'] = df['memory_gb'].diff()
        return df

    def plot_memory_usage(self, output_path: str):
        """ç»˜åˆ¶å†…å­˜ä½¿ç”¨è¶‹åŠ¿å›¾"""
        df = self.get_summary()
        if len(df) == 0:
            return

        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(range(len(df)), df['memory_gb'], marker='o', linewidth=2, markersize=8)
        ax.set_xticks(range(len(df)))
        ax.set_xticklabels(df['step'], rotation=45, ha='right')
        ax.set_ylabel('Memory Usage (GB)', fontsize=12)
        ax.set_xlabel('Pipeline Step', fontsize=12)
        ax.set_title('Memory Usage Throughout Pipeline', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"   ğŸ’¾ å†…å­˜ä½¿ç”¨å›¾å·²ä¿å­˜: {output_path}")


def safe_step_execution(checkpoint_manager: CheckpointManager, step_name: str,
                       func, *args, **kwargs):
    """
    å®‰å…¨æ‰§è¡Œæµç¨‹æ­¥éª¤ï¼Œè‡ªåŠ¨å¤„ç†é”™è¯¯å’Œæ–­ç‚¹ä¿å­˜

    å‚æ•°:
    --------
    checkpoint_manager: CheckpointManager
        æ–­ç‚¹ç®¡ç†å™¨
    step_name: str
        æ­¥éª¤åç§°
    func: callable
        è¦æ‰§è¡Œçš„å‡½æ•°
    *args, **kwargs:
        ä¼ é€’ç»™å‡½æ•°çš„å‚æ•°

    è¿”å›:
    --------
    result: å‡½æ•°æ‰§è¡Œç»“æœ
    """
    try:
        print(f"\n{'='*80}")
        print(f"å¼€å§‹æ‰§è¡Œ: {step_name}")
        print(f"{'='*80}")

        result = func(*args, **kwargs)

        print(f"âœ… {step_name} æ‰§è¡ŒæˆåŠŸ")
        return result

    except KeyboardInterrupt:
        print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        checkpoint_manager.mark_step_failed(step_name,
                                           Exception("User interrupted"))
        sys.exit(1)

    except Exception as e:
        print(f"\nâŒ {step_name} æ‰§è¡Œå¤±è´¥!")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")

        # è®°å½•å¤±è´¥ä¿¡æ¯
        checkpoint_manager.mark_step_failed(step_name, e)

        # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
        print(f"\nè¯¦ç»†é”™è¯¯è¿½è¸ª:")
        print(traceback.format_exc())

        print(f"\nğŸ’¡ å¯ä»¥ä¿®å¤é—®é¢˜åä½¿ç”¨ --resume å‚æ•°ä»æ–­ç‚¹ç»§ç»­è¿è¡Œ")

        sys.exit(1)


def readbgi(data_dir: str, sample_id: str,
           memory_monitor: Optional[MemoryMonitor] = None) -> ad.AnnData:
    """
    ä» BGI (æˆ– 10X) è¾“å‡ºç›®å½•è¯»å–å•ç»†èƒçŸ©é˜µæ•°æ®ï¼Œç”Ÿæˆ AnnData å¯¹è±¡ã€‚

    å‚æ•°ï¼š
    ----------
    data_dir : str
        å­˜æ”¾ matrix.mtx(.gz)ã€barcodes.tsv(.gz)ã€features.tsv(.gz) çš„ç›®å½•è·¯å¾„
    sample_id : str
        æ ·å“ IDï¼Œç”¨äºä¸ºç»†èƒæ¡ç æ·»åŠ å‰ç¼€ä»¥é¿å…é‡å¤
    memory_monitor : MemoryMonitor, optional
        å†…å­˜ç›‘æ§å¯¹è±¡

    è¿”å›ï¼š
    ----------
    adata : AnnData
        åŒ…å«è¡¨è¾¾çŸ©é˜µä¸æ ·å“æ ‡è¯†çš„ AnnData å¯¹è±¡
    """
    # è‡ªåŠ¨æŸ¥æ‰¾ä¸‰ä¸ªæ–‡ä»¶ï¼ˆæ”¯æŒå‹ç¼©æ–‡ä»¶ï¼‰
    def find_file(name_starts):
        for fn in os.listdir(data_dir):
            if fn.startswith(name_starts) and (fn.endswith(".tsv") or fn.endswith(".tsv.gz") or
                                               fn.endswith(".mtx") or fn.endswith(".mtx.gz")):
                return os.path.join(data_dir, fn)
        raise FileNotFoundError(f"æœªæ‰¾åˆ° {name_starts} æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼š{data_dir}")

    try:
        matrix_path = find_file("matrix")
        barcodes_path = find_file("barcodes")
        features_path = find_file("features")

        # è¯»å–æ•°æ®
        matrix = io.mmread(matrix_path)
        barcodes = pd.read_csv(barcodes_path, header=None, sep='\t')
        features = pd.read_csv(features_path, header=None, sep='\t')

        # ç»™ç»†èƒæ¡ç åŠ æ ·å“å‰ç¼€
        barcodes[0] = barcodes[0].apply(lambda x: f"{sample_id}_{x}")

        # è½¬ç½®ä¸ºç»†èƒÃ—åŸºå› çŸ©é˜µå¹¶è½¬æ¢ä¸ºç¨€ç–æ ¼å¼
        adata = sc.AnnData(X=csr_matrix(matrix.T))
        adata.obs_names = barcodes[0].values
        adata.var_names = features[0].values
        adata.obs["SampleName"] = sample_id

        # ç¡®ä¿åŸºå› åå”¯ä¸€
        adata.var_names_make_unique()

        return adata

    except Exception as e:
        print(f"âŒ è¯»å–æ ·å“ {sample_id} å¤±è´¥: {str(e)}")
        raise


def read_sample(sample_info_path: str,
               memory_monitor: Optional[MemoryMonitor] = None) -> ad.AnnData:
    """
    æ ¹æ®æ ·å“ä¿¡æ¯æ–‡ä»¶æ‰¹é‡è¯»å–å¹¶æ•´åˆå•ç»†èƒæ•°æ®

    å‚æ•°ï¼š
    ----------
    sample_info_path : str
        æ ·å“ä¿¡æ¯ CSV æ–‡ä»¶è·¯å¾„
    æ ¼å¼è¦æ±‚ï¼š
        å¿…é¡»åŒ…å« 'Path' å’Œ 'SampleName' ä¸¤åˆ—ï¼Œå…¶ä»–åˆ—å°†ä½œä¸ºå…ƒæ•°æ®æ·»åŠ åˆ° obs ä¸­ã€‚
    memory_monitor : MemoryMonitor, optional
        å†…å­˜ç›‘æ§å¯¹è±¡

    è¿”å›ï¼š
    ----------
    adata : AnnData
        æ•´åˆåçš„ AnnData å¯¹è±¡
    """
    # è¯»å–æ ·å“ä¿¡æ¯
    sample_info = pd.read_csv(sample_info_path)

    # éªŒè¯å¿…éœ€åˆ—
    required_cols = ['Path', 'SampleName']
    missing_cols = [col for col in required_cols if col not in sample_info.columns]
    if missing_cols:
        raise ValueError(f"æ ·å“ä¿¡æ¯æ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")

    adata_list = []

    # è¯»å–å„æ ·å“æ•°æ®
    for idx, row in tqdm(sample_info.iterrows(), total=len(sample_info), desc="ğŸ“– è¯»å–æ ·å“"):
        sample_id = row['SampleName']
        data_dir = row['Path']

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_dir):
            print(f"âš ï¸  è­¦å‘Š: è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡æ ·å“ {sample_id}: {data_dir}")
            continue

        try:
            # è¯»å–å•ä¸ªæ ·å“æ•°æ®
            adata_sample = readbgi(data_dir, sample_id, memory_monitor)

            # æ·»åŠ å…ƒæ•°æ®
            for col in sample_info.columns:
                if col not in ['Path', 'SampleName']:
                    adata_sample.obs[col] = row[col]

            adata_list.append(adata_sample)

        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: è·³è¿‡æ ·å“ {sample_id}: {str(e)}")
            continue

    if len(adata_list) == 0:
        raise ValueError("âŒ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ ·å“æ•°æ®ï¼")

    # åˆå¹¶æ‰€æœ‰æ ·å“
    print(f"\nğŸ”— åˆå¹¶ {len(adata_list)} ä¸ªæ ·å“...")
    adata = ad.concat(adata_list, join='outer', merge='same')

    if memory_monitor:
        memory_monitor.checkpoint("æ•°æ®è¯»å–å®Œæˆ")

    # é‡Šæ”¾å†…å­˜
    del adata_list
    gc.collect()

    return adata


def plot_qc_metrics(adata: ad.AnnData, output_dir: str, prefix: str = "before_qc"):
    """
    ç»˜åˆ¶è´¨æ§æŒ‡æ ‡å¯è§†åŒ–å›¾

    å‚æ•°ï¼š
    ----------
    adata : AnnData
        åŒ…å«è´¨æ§æŒ‡æ ‡çš„ AnnData å¯¹è±¡
    output_dir : str
        è¾“å‡ºç›®å½•
    prefix : str
        æ–‡ä»¶åå‰ç¼€ï¼ˆbefore_qc æˆ– after_qcï¼‰
    """
    print(f"\n   ğŸ“Š ç”Ÿæˆè´¨æ§å¯è§†åŒ–å›¾ ({prefix})...")

    # åˆ›å»º QC å­ç›®å½•
    qc_dir = os.path.join(output_dir, "qc_plots")
    os.makedirs(qc_dir, exist_ok=True)

    # 1. å°æç´å›¾ - ä¸»è¦QCæŒ‡æ ‡
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # n_genes_by_counts
    sc.pl.violin(adata, 'n_genes_by_counts', groupby='SampleName',
                 rotation=45, ax=axes[0], show=False)
    axes[0].set_title('Genes per Cell', fontweight='bold')
    axes[0].set_ylabel('Number of Genes')

    # total_counts
    sc.pl.violin(adata, 'total_counts', groupby='SampleName',
                 rotation=45, ax=axes[1], show=False)
    axes[1].set_title('UMI Counts per Cell', fontweight='bold')
    axes[1].set_ylabel('Total UMI Counts')

    # pct_counts_mt
    sc.pl.violin(adata, 'pct_counts_mt', groupby='SampleName',
                 rotation=45, ax=axes[2], show=False)
    axes[2].set_title('Mitochondrial %', fontweight='bold')
    axes[2].set_ylabel('% Mitochondrial Genes')

    plt.tight_layout()
    violin_path = os.path.join(qc_dir, f'{prefix}_violin.png')
    plt.savefig(violin_path, dpi=300, bbox_inches='tight')
    plt.close()

    # 2. æ•£ç‚¹å›¾ - åŸºå› æ•° vs UMIæ•°
    fig, ax = plt.subplots(figsize=(10, 8))
    sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts',
                  color='pct_counts_mt', ax=ax, show=False)
    ax.set_title('QC Metrics: Genes vs UMI Counts', fontweight='bold', fontsize=14)
    scatter_path = os.path.join(qc_dir, f'{prefix}_scatter.png')
    plt.savefig(scatter_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"   âœ… QCå¯è§†åŒ–å›¾å·²ä¿å­˜è‡³: {qc_dir}/")


def plot_qc_comparison(adata_after: ad.AnnData, adata_before: ad.AnnData, output_dir: str):
    """
    ç»˜åˆ¶è´¨æ§å‰åå¯¹æ¯”å›¾

    å‚æ•°ï¼š
    ----------
    adata_after : AnnData
        è´¨æ§åçš„æ•°æ®
    adata_before : AnnData
        è´¨æ§å‰çš„æ•°æ®
    output_dir : str
        è¾“å‡ºç›®å½•
    """
    qc_dir = os.path.join(output_dir, "qc_plots")

    # è·å–è´¨æ§å‰åçš„æ•°æ®
    before_df = adata_before.obs.copy()
    after_df = adata_after.obs.copy()

    # åˆ›å»ºå¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))

    metrics = ['n_genes_by_counts', 'total_counts', 'pct_counts_mt']
    titles = ['Genes per Cell', 'UMI Counts', 'Mitochondrial %']

    for idx, (metric, title) in enumerate(zip(metrics, titles)):
        # è´¨æ§å‰
        axes[0, idx].hist(before_df[metric], bins=50, alpha=0.7,
                         edgecolor='black', color='lightcoral')
        axes[0, idx].set_title(f'{title} (Before QC)', fontweight='bold')
        axes[0, idx].set_xlabel(metric)
        axes[0, idx].set_ylabel('Number of Cells')

        # è´¨æ§å
        axes[1, idx].hist(after_df[metric], bins=50, alpha=0.7,
                         edgecolor='black', color='lightgreen')
        axes[1, idx].set_title(f'{title} (After QC)', fontweight='bold')
        axes[1, idx].set_xlabel(metric)
        axes[1, idx].set_ylabel('Number of Cells')

    plt.tight_layout()
    comparison_path = os.path.join(qc_dir, 'qc_comparison.png')
    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"   âœ… è´¨æ§å¯¹æ¯”å›¾å·²ä¿å­˜: {comparison_path}")


def quality_control(
    adata: sc.AnnData,
    min_genes: int = 500,
    max_genes: int = 8000,
    max_pct_mito: float = 15,
    min_counts: int = 1000,
    doublet_method: str = "scrublet",
    doublet_threshold: float = 0.25,
    output_dir: str = "./results",
    return_stats: bool = True,
    memory_monitor: Optional[MemoryMonitor] = None
) -> Union[sc.AnnData, Tuple[sc.AnnData, pd.DataFrame]]:
    """
    å•ç»†èƒæ•°æ®è´¨æ§æµç¨‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    """
    print("=" * 80)
    print("å¼€å§‹è´¨æ§æµç¨‹...")
    print("=" * 80)

    # 1. è®¡ç®—QCæŒ‡æ ‡
    print("\n[1/5] è®¡ç®—è´¨æ§æŒ‡æ ‡...")

    # è¯†åˆ«çº¿ç²’ä½“åŸºå› ï¼ˆæ”¯æŒäººå’Œé¼ ï¼‰
    adata.var['mt'] = (adata.var_names.str.startswith('MT-') |
                       adata.var_names.str.startswith('mt-') |
                       adata.var_names.str.startswith('Mt-'))

    # è¯†åˆ«æ ¸ç³–ä½“åŸºå› ï¼ˆå¯é€‰ï¼‰
    adata.var['ribo'] = (adata.var_names.str.startswith('RPS') |
                         adata.var_names.str.startswith('RPL') |
                         adata.var_names.str.startswith('Rps') |
                         adata.var_names.str.startswith('Rpl'))

    # è®¡ç®—QC metrics
    sc.pp.calculate_qc_metrics(
        adata,
        qc_vars=['mt', 'ribo'],
        percent_top=None,
        log1p=False,
        inplace=True
    )

    print(f"   âœ“ è´¨æ§æŒ‡æ ‡è®¡ç®—å®Œæˆ")
    print(f"   - çº¿ç²’ä½“åŸºå› æ•°: {adata.var['mt'].sum()}")
    print(f"   - æ ¸ç³–ä½“åŸºå› æ•°: {adata.var['ribo'].sum()}")

    if memory_monitor:
        memory_monitor.checkpoint("QCæŒ‡æ ‡è®¡ç®—")

    # 2. ç»˜åˆ¶è´¨æ§å‰çš„å›¾
    print("\n[2/5] ç”Ÿæˆè´¨æ§å‰å¯è§†åŒ–å›¾...")
    plot_qc_metrics(adata, output_dir, prefix="before_qc")

    # è®°å½•è¿‡æ»¤å‰çš„ç»†èƒæ•°
    samples = adata.obs['SampleName'].unique()
    qc_stats_list = []

    for sample in samples:
        sample_mask = adata.obs['SampleName'] == sample
        sample_data = adata.obs[sample_mask]
        qc_stats_list.append({
            'SampleName': sample,
            'cells_before_qc': sample_mask.sum(),
            'median_genes_before': sample_data['n_genes_by_counts'].median(),
            'median_counts_before': sample_data['total_counts'].median(),
            'median_mito_before': sample_data['pct_counts_mt'].median()
        })

    # 3. å»é™¤åŒèƒ
    if doublet_method.lower() == "scrublet":
        print(f"\n[3/5] ä½¿ç”¨ Scrublet æ£€æµ‹åŒèƒ (é˜ˆå€¼={doublet_threshold})...")

        import scrublet as scr

        doublet_scores = []
        doublet_labels = []

        for sample in tqdm(samples, desc="   æ£€æµ‹åŒèƒ"):
            sample_mask = adata.obs['SampleName'] == sample
            adata_sample = adata[sample_mask].copy()

            try:
                # è¿è¡Œ Scrublet
                scrub = scr.Scrublet(adata_sample.X)
                doublet_score, predicted_doublet = scrub.scrub_doublets(
                    min_counts=2,
                    min_cells=3,
                    min_gene_variability_pctl=85,
                    n_prin_comps=30,
                    verbose=False
                )

                # ä½¿ç”¨è‡ªå®šä¹‰é˜ˆå€¼
                predicted_doublet = doublet_score > doublet_threshold

                doublet_scores.extend(doublet_score)
                doublet_labels.extend(predicted_doublet)

            except Exception as e:
                print(f"   âš ï¸  æ ·å“ {sample} åŒèƒæ£€æµ‹å¤±è´¥: {str(e)}")
                # å¦‚æœå¤±è´¥ï¼Œæ ‡è®°æ‰€æœ‰ç»†èƒä¸ºéåŒèƒ
                doublet_scores.extend([0.0] * sample_mask.sum())
                doublet_labels.extend([False] * sample_mask.sum())

        adata.obs['doublet_score'] = doublet_scores
        adata.obs['is_doublet'] = doublet_labels

        n_doublets = sum(doublet_labels)
        print(f"   âœ“ æ£€æµ‹åˆ° {n_doublets:,} ä¸ªåŒèƒ ({n_doublets/len(adata)*100:.2f}%)")
    else:
        print("\n[3/5] è·³è¿‡åŒèƒæ£€æµ‹...")
        adata.obs['is_doublet'] = False

    if memory_monitor:
        memory_monitor.checkpoint("åŒèƒæ£€æµ‹")

    # 4. åº”ç”¨è¿‡æ»¤æ¡ä»¶
    print(f"\n[4/5] åº”ç”¨è¿‡æ»¤æ¡ä»¶...")
    print(f"   - æœ€ä½åŸºå› æ•°: {min_genes}")
    print(f"   - æœ€é«˜åŸºå› æ•°: {max_genes}")
    print(f"   - æœ€ä½UMIæ•°: {min_counts}")
    print(f"   - æœ€å¤§çº¿ç²’ä½“æ¯”ä¾‹: {max_pct_mito}%")

    # åˆ›å»ºè¿‡æ»¤mask
    adata.obs['pass_qc'] = (
        (adata.obs['n_genes_by_counts'] >= min_genes) &
        (adata.obs['n_genes_by_counts'] <= max_genes) &
        (adata.obs['total_counts'] >= min_counts) &
        (adata.obs['pct_counts_mt'] <= max_pct_mito) &
        (~adata.obs['is_doublet'])
    )

    # 5. ç»Ÿè®¡æ¯ä¸ªæ ·å“çš„è¿‡æ»¤æƒ…å†µ
    print("\n[5/5] ç»Ÿè®¡è¿‡æ»¤ç»“æœ...")

    for i, sample in enumerate(samples):
        sample_mask = adata.obs['SampleName'] == sample
        sample_data = adata.obs[sample_mask]

        n_cells_before = qc_stats_list[i]['cells_before_qc']
        n_cells_after = sample_data['pass_qc'].sum()
        n_filtered = n_cells_before - n_cells_after
        pct_filtered = n_filtered / n_cells_before * 100 if n_cells_before > 0 else 0

        # ç»Ÿè®¡å„ç§è¿‡æ»¤åŸå› 
        n_low_genes = (sample_data['n_genes_by_counts'] < min_genes).sum()
        n_high_genes = (sample_data['n_genes_by_counts'] > max_genes).sum()
        n_low_counts = (sample_data['total_counts'] < min_counts).sum()
        n_high_mito = (sample_data['pct_counts_mt'] > max_pct_mito).sum()
        n_doublet = sample_data['is_doublet'].sum()

        # è´¨æ§åçš„ç»Ÿè®¡
        passed_data = sample_data[sample_data['pass_qc']]

        qc_stats_list[i].update({
            'cells_after_qc': n_cells_after,
            'cells_filtered': n_filtered,
            'pct_filtered': f"{pct_filtered:.2f}%",
            'n_low_genes': n_low_genes,
            'n_high_genes': n_high_genes,
            'n_low_counts': n_low_counts,
            'n_high_mito': n_high_mito,
            'n_doublet': n_doublet,
            'median_genes_after': passed_data['n_genes_by_counts'].median() if len(passed_data) > 0 else 0,
            'median_counts_after': passed_data['total_counts'].median() if len(passed_data) > 0 else 0,
            'median_mito_after': passed_data['pct_counts_mt'].median() if len(passed_data) > 0 else 0
        })

    qc_stats_df = pd.DataFrame(qc_stats_list)

    # è¾“å‡ºç»Ÿè®¡æ‘˜è¦
    print("\n" + "=" * 80)
    print("è´¨æ§ç»Ÿè®¡æ‘˜è¦:")
    print("=" * 80)
    print(qc_stats_df.to_string(index=False))
    print("=" * 80)

    # è¿‡æ»¤æ•°æ®å¹¶æ­£ç¡®ä¿ç•™ .raw
    print(f"\næ­£åœ¨è¿‡æ»¤æ•°æ®...")
    print(f"   è¿‡æ»¤å‰: {adata.n_obs:,} ç»†èƒ")

    # ä¿å­˜è´¨æ§å‰çš„æ•°æ®ç”¨äºå¯¹æ¯”å›¾
    adata_before_qc = adata.copy()

    # ä¿å­˜åŸå§‹countsåˆ° .raw
    adata.raw = adata

    # è¿‡æ»¤ç»†èƒ
    adata = adata[adata.obs['pass_qc']].copy()

    # è¿‡æ»¤ä½è¡¨è¾¾åŸºå› ï¼ˆåœ¨è‡³å°‘3ä¸ªç»†èƒä¸­è¡¨è¾¾ï¼‰
    sc.pp.filter_genes(adata, min_cells=3)

    print(f"   è¿‡æ»¤å: {adata.n_obs:,} ç»†èƒ, {adata.n_vars:,} åŸºå› ")
    print(f"   è¿‡æ»¤ç‡: {(1 - adata.n_obs/adata.raw.n_obs)*100:.2f}%")

    if memory_monitor:
        memory_monitor.checkpoint("æ•°æ®è¿‡æ»¤")

    # ç»˜åˆ¶è´¨æ§åçš„å›¾
    print("\nç”Ÿæˆè´¨æ§åå¯è§†åŒ–å›¾...")
    plot_qc_metrics(adata, output_dir, prefix="after_qc")

    # ç”Ÿæˆå¯¹æ¯”å›¾
    print("\nç”Ÿæˆè´¨æ§å‰åå¯¹æ¯”å›¾...")
    plot_qc_comparison(adata, adata_before_qc, output_dir)

    print("\n" + "=" * 80)
    print("âœ… è´¨æ§å®Œæˆ!")
    print("=" * 80)

    # æ¸…ç†å†…å­˜
    gc.collect()

    if return_stats:
        return adata, qc_stats_df
    else:
        return adata


def data_integration(
    adata: sc.AnnData,
    batch_key: str = "SampleName",
    method: str = "harmony",
    n_pcs: int = 50,
    n_neighbors: int = 30,
    resolution: float = 0.8,
    gene_num: int = 3000,
    umap_min_dist: float = 0.3,
    output_dir: str = "./results",
    memory_monitor: Optional[MemoryMonitor] = None
) -> sc.AnnData:
    """
    æ•°æ®æ•´åˆä¸é™ç»´èšç±»ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    """
    print("\n" + "=" * 80)
    print(f"æ•°æ®æ•´åˆä¸é™ç»´ (æ–¹æ³•: {method})")
    print("=" * 80)

    # 1. æ ‡å‡†åŒ–å’Œå¯¹æ•°è½¬æ¢
    print("\n[1/7] æ•°æ®æ ‡å‡†åŒ–...")
    sc.pp.normalize_total(adata, target_sum=1e4)
    sc.pp.log1p(adata)

    if memory_monitor:
        memory_monitor.checkpoint("æ•°æ®æ ‡å‡†åŒ–")

    # 2. é‰´å®šé«˜å˜åŸºå› 
    print(f"\n[2/7] é‰´å®šé«˜å˜åŸºå›  (n={gene_num})...")
    sc.pp.highly_variable_genes(
        adata,
        n_top_genes=gene_num,
        batch_key=batch_key,
        flavor='seurat_v3',
        subset=False
    )

    n_hvg = adata.var['highly_variable'].sum()
    print(f"   âœ“ é‰´å®šåˆ° {n_hvg} ä¸ªé«˜å˜åŸºå› ")

    # 3. æ•°æ®ç¼©æ”¾
    print("\n[3/7] æ•°æ®ç¼©æ”¾...")
    sc.pp.scale(adata, max_value=10)

    if memory_monitor:
        memory_monitor.checkpoint("æ•°æ®ç¼©æ”¾")

    # 4. PCAé™ç»´
    print(f"\n[4/7] PCAé™ç»´ (n_pcs={n_pcs})...")
    sc.tl.pca(adata, svd_solver='arpack', n_comps=n_pcs)

    # ç»˜åˆ¶PCAæ–¹å·®è§£é‡Šå›¾
    fig, ax = plt.subplots(figsize=(10, 6))
    variance_ratio = adata.uns['pca']['variance_ratio']
    cumsum_variance = np.cumsum(variance_ratio)

    ax.plot(range(1, len(variance_ratio)+1), variance_ratio, 'o-', label='Individual')
    ax.plot(range(1, len(cumsum_variance)+1), cumsum_variance, 's-', label='Cumulative')
    ax.axhline(0.9, color='r', linestyle='--', label='90% variance')
    ax.set_xlabel('Principal Component')
    ax.set_ylabel('Variance Explained')
    ax.set_title('PCA Variance Explained')
    ax.legend()
    ax.grid(True, alpha=0.3)

    pca_path = os.path.join(output_dir, 'pca_variance.png')
    plt.savefig(pca_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"   âœ“ PCAå®Œæˆï¼Œå‰{n_pcs}ä¸ªPCè§£é‡Š {cumsum_variance[n_pcs-1]*100:.2f}% çš„æ–¹å·®")

    if memory_monitor:
        memory_monitor.checkpoint("PCAé™ç»´")

    # 5. æ‰¹æ¬¡æ•ˆåº”æ ¡æ­£
    if method.lower() == "harmony":
        print("\n[5/7] Harmony æ‰¹æ¬¡æ ¡æ­£...")
        try:
            import harmonypy as hm
            ho = hm.run_harmony(
                adata.obsm['X_pca'],
                adata.obs,
                batch_key,
                max_iter_harmony=20,
                verbose=False
            )
            adata.obsm['X_pca_harmony'] = ho.Z_corr.T
            use_rep = 'X_pca_harmony'
            print("   âœ“ Harmony æ ¡æ­£å®Œæˆ")
        except ImportError:
            print("   âš ï¸  harmonypy æœªå®‰è£…ï¼Œè·³è¿‡æ‰¹æ¬¡æ ¡æ­£")
            use_rep = 'X_pca'
        except Exception as e:
            print(f"   âš ï¸  Harmony æ ¡æ­£å¤±è´¥: {str(e)}")
            use_rep = 'X_pca'
    else:
        print("\n[5/7] è·³è¿‡æ‰¹æ¬¡æ ¡æ­£...")
        use_rep = 'X_pca'

    if memory_monitor:
        memory_monitor.checkpoint("æ‰¹æ¬¡æ ¡æ­£")

    # 6. æ„å»ºé‚»å±…å›¾å’ŒUMAP
    print(f"\n[6/7] æ„å»ºé‚»å±…å›¾ (n_neighbors={n_neighbors})...")
    sc.pp.neighbors(adata, n_neighbors=n_neighbors, n_pcs=n_pcs, use_rep=use_rep)

    print(f"   è®¡ç®— UMAP (min_dist={umap_min_dist})...")
    sc.tl.umap(adata, min_dist=umap_min_dist)

    if memory_monitor:
        memory_monitor.checkpoint("é‚»å±…å›¾å’ŒUMAP")

    # 7. Leidenèšç±»
    print(f"\n[7/7] Leiden èšç±» (resolution={resolution})...")
    sc.tl.leiden(adata, resolution=resolution, key_added='leiden')

    n_clusters = adata.obs['leiden'].nunique()
    print(f"   âœ“ é‰´å®šåˆ° {n_clusters} ä¸ªèšç±»")

    if memory_monitor:
        memory_monitor.checkpoint("èšç±»å®Œæˆ")

    # 8. ç”Ÿæˆå¯è§†åŒ–å›¾
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾...")
    plot_integration_results(adata, output_dir, batch_key)

    print("\n" + "=" * 80)
    print("âœ… æ•°æ®æ•´åˆå®Œæˆ!")
    print("=" * 80)

    # æ¸…ç†å†…å­˜
    gc.collect()

    return adata


def plot_integration_results(adata: ad.AnnData, output_dir: str, batch_key: str):
    """
    ç»˜åˆ¶æ•´åˆç»“æœå¯è§†åŒ–å›¾
    """
    # 1. UMAP - æŒ‰èšç±»ç€è‰²
    sc.pl.umap(adata, color='leiden', legend_loc='on data',
               title='UMAP - Leiden Clusters', frameon=False,
               save='_leiden.png')

    # 2. UMAP - æŒ‰æ ·å“ç€è‰²
    sc.pl.umap(adata, color=batch_key, title=f'UMAP - {batch_key}',
               frameon=False, save=f'_{batch_key}.png')

    print(f"   âœ… å¯è§†åŒ–å›¾å·²ä¿å­˜è‡³ {output_dir}/figures/")


def celltypist_annotation(
    adata: sc.AnnData,
    model_path: Optional[str] = None,
    output_path: str = "./annotated_data.h5ad"
) -> sc.AnnData:
    """
    ä½¿ç”¨ CellTypist è¿›è¡Œç»†èƒç±»å‹æ³¨é‡Š
    """
    try:
        import celltypist
        from celltypist import models

        if model_path is None:
            print("   ä¸‹è½½é»˜è®¤ CellTypist æ¨¡å‹...")
            model = models.Model.load(model='Immune_All_Low.pkl')
        else:
            print(f"   åŠ è½½ CellTypist æ¨¡å‹: {model_path}")
            model = models.Model.load(model=model_path)

        print("   å¼€å§‹ç»†èƒç±»å‹æ³¨é‡Š...")
        predictions = celltypist.annotate(adata, model=model, majority_voting=True)

        result_model = predictions.predicted_labels[['predicted_labels', 'majority_voting']].rename(
            columns={'predicted_labels': 'celltype_pred', 'majority_voting': 'celltype_pred_mv'}
        )

        adata.obs = adata.obs.join(result_model, how='left')

        print(f"   âœ“ æ³¨é‡Šå®Œæˆï¼Œç»“æœä¿å­˜åˆ° {output_path}")
        adata.write_h5ad(output_path)

        return adata

    except ImportError:
        print("   âŒ celltypist æœªå®‰è£…ï¼Œè·³è¿‡ç»†èƒç±»å‹æ³¨é‡Š")
        print("   å®‰è£…å‘½ä»¤: pip install celltypist")
        return adata
    except Exception as e:
        print(f"   âŒ ç»†èƒç±»å‹æ³¨é‡Šå¤±è´¥: {str(e)}")
        return adata


def get_args():
    import argparse

    parser = argparse.ArgumentParser(
        description="ä¼˜åŒ–çš„å•ç»†èƒRNA-seqæ•°æ®å¤„ç†æµç¨‹ (æ”¯æŒæ–­ç‚¹ç»­ä¼ )",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # å¿…éœ€å‚æ•°
    parser.add_argument("--sample_info", required=True,
                       help="æ ·å“ä¿¡æ¯CSVæ–‡ä»¶è·¯å¾„ (å¿…é¡»åŒ…å« Path å’Œ SampleName åˆ—)")
    parser.add_argument("--output_dir", default="./results",
                       help="è¾“å‡ºç›®å½•")

    # æ–­ç‚¹æ§åˆ¶å‚æ•°
    checkpoint_group = parser.add_argument_group('æ–­ç‚¹ç»­ä¼ æ§åˆ¶')
    checkpoint_group.add_argument("--resume", action='store_true',
                                 help="ä»ä¸Šæ¬¡æ–­ç‚¹ç»§ç»­è¿è¡Œ")
    checkpoint_group.add_argument("--auto-resume", action='store_true',
                                 help="è‡ªåŠ¨ä»æ–­ç‚¹æ¢å¤ï¼ˆæ— éœ€ç¡®è®¤ï¼‰")
    checkpoint_group.add_argument("--reset", action='store_true',
                                 help="é‡ç½®æ‰€æœ‰æ–­ç‚¹ï¼Œä»å¤´å¼€å§‹")
    checkpoint_group.add_argument("--show-progress", action='store_true',
                                 help="åªæ˜¾ç¤ºå½“å‰è¿›åº¦ï¼Œä¸è¿è¡Œ")

    # è´¨æ§å‚æ•°ï¼ˆä¼˜åŒ–åçš„é»˜è®¤å€¼ï¼‰
    qc_group = parser.add_argument_group('è´¨æ§å‚æ•°')
    qc_group.add_argument("--min_genes", type=int, default=500,
                         help="æ¯ä¸ªç»†èƒçš„æœ€ä½åŸºå› æ•°")
    qc_group.add_argument("--max_genes", type=int, default=8000,
                         help="æ¯ä¸ªç»†èƒçš„æœ€é«˜åŸºå› æ•°")
    qc_group.add_argument("--min_counts", type=int, default=1000,
                         help="æ¯ä¸ªç»†èƒçš„æœ€ä½UMIæ•°")
    qc_group.add_argument("--max_pct_mito", type=float, default=15.0,
                         help="æœ€å¤§çº¿ç²’ä½“åŸºå› ç™¾åˆ†æ¯”")
    qc_group.add_argument("--doublet_method", default="scrublet",
                         choices=["scrublet", "none"],
                         help="åŒèƒæ£€æµ‹æ–¹æ³•")
    qc_group.add_argument("--doublet_threshold", type=float, default=0.25,
                         help="åŒèƒæ£€æµ‹é˜ˆå€¼")

    # æ•´åˆä¸èšç±»å‚æ•°ï¼ˆä¼˜åŒ–åçš„é»˜è®¤å€¼ï¼‰
    int_group = parser.add_argument_group('æ•´åˆä¸èšç±»å‚æ•°')
    int_group.add_argument("--integration_method", default="harmony",
                          choices=["harmony", "combat", "scanorama", "none"],
                          help="æ‰¹æ¬¡æ ¡æ­£æ–¹æ³•")
    int_group.add_argument("--n_pcs", type=int, default=50,
                          help="ä¸»æˆåˆ†æ•°é‡")
    int_group.add_argument("--n_neighbors", type=int, default=30,
                          help="é‚»å±…æ•°é‡")
    int_group.add_argument("--resolution", type=float, default=0.8,
                          help="Leidenèšç±»åˆ†è¾¨ç‡")
    int_group.add_argument("--gene_num", type=int, default=3000,
                          help="é«˜å˜åŸºå› æ•°é‡")
    int_group.add_argument("--umap_min_dist", type=float, default=0.3,
                          help="UMAPæœ€å°è·ç¦»")

    # æ³¨é‡Šå‚æ•°
    ann_group = parser.add_argument_group('ç»†èƒç±»å‹æ³¨é‡Šå‚æ•°')
    ann_group.add_argument("--run_annotation", action='store_true',
                          help="è¿è¡ŒCellTypistç»†èƒç±»å‹æ³¨é‡Š")
    ann_group.add_argument("--celltypist_model", default=None,
                          help="CellTypistæ¨¡å‹è·¯å¾„")

    # å…¶ä»–å‚æ•°
    parser.add_argument("--n_jobs", type=int, default=-1,
                       help="å¹¶è¡Œä»»åŠ¡æ•° (-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰CPU)")

    return parser.parse_args()


def main():
    """
    å•ç»†èƒRNA-seqæ•°æ®å¤„ç†ä¸»æµç¨‹ï¼ˆå¢å¼ºç‰ˆ - æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    """
    args = get_args()

    # è®¾ç½® scanpy å‚æ•°
    sc.settings.verbosity = 1
    sc.settings.n_jobs = args.n_jobs
    sc.settings.figdir = os.path.join(args.output_dir, 'figures')

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs(sc.settings.figdir, exist_ok=True)

    # åˆå§‹åŒ–æ–­ç‚¹ç®¡ç†å™¨
    ckpt = CheckpointManager(args.output_dir, auto_resume=args.auto_resume)

    # å¤„ç†æ–­ç‚¹ç›¸å…³å‘½ä»¤
    if args.reset:
        ckpt.reset()

    if args.show_progress:
        ckpt.print_status()
        return

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä»æ–­ç‚¹æ¢å¤
    if args.resume or args.auto_resume:
        if ckpt.should_resume():
            pass  # ç»§ç»­æ‰§è¡Œ
        elif not ckpt.get_last_completed_step():
            print("æ²¡æœ‰æ£€æµ‹åˆ°ä¹‹å‰çš„è¿›åº¦ï¼Œå°†ä»å¤´å¼€å§‹è¿è¡Œ\n")

    # åˆå§‹åŒ–å†…å­˜ç›‘æ§
    mem_monitor = MemoryMonitor()
    mem_monitor.checkpoint("æµç¨‹å¼€å§‹")

    print("\n" + "=" * 100)
    print(" " * 30 + "å•ç»†èƒRNA-seqæ•°æ®å¤„ç†æµç¨‹ (å¢å¼ºç‰ˆ v2.2)")
    print("=" * 100)
    print(f"\nğŸ“‹ é…ç½®å‚æ•°ï¼š")
    print(f"  æ ·å“ä¿¡æ¯: {args.sample_info}")
    print(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"  æ–­ç‚¹æ–‡ä»¶: {ckpt.checkpoint_file}")
    print("=" * 100 + "\n")

    # ============================================
    # æ­¥éª¤ 1: è¯»å–æ ·å“æ•°æ®
    # ============================================
    raw_data_path = os.path.join(args.output_dir, "01_raw_data.h5ad")

    if not ckpt.is_completed('step1_read'):
        def read_data_step():
            adata = read_sample(args.sample_info, memory_monitor=mem_monitor)

            # ä¿å­˜åŸå§‹æ•°æ®
            adata.write(raw_data_path)

            print(f"\nâœ… è¯»å–å®Œæˆ:")
            print(f"  ç»†èƒæ•°: {adata.n_obs:,}")
            print(f"  åŸºå› æ•°: {adata.n_vars:,}")
            print(f"  æ•°æ®å·²ä¿å­˜: {raw_data_path}")

            # ä¿å­˜æ–­ç‚¹
            ckpt.save_checkpoint('step1_read',
                               file=raw_data_path,
                               n_cells=adata.n_obs,
                               n_genes=adata.n_vars)

            return adata

        adata = safe_step_execution(ckpt, 'æ­¥éª¤1: è¯»å–æ ·å“æ•°æ®', read_data_step)
    else:
        print("\nâœ“ æ­¥éª¤1å·²å®Œæˆï¼ŒåŠ è½½å·²æœ‰æ•°æ®...")
        adata = sc.read_h5ad(raw_data_path)
        print(f"  ç»†èƒæ•°: {adata.n_obs:,}")
        print(f"  åŸºå› æ•°: {adata.n_vars:,}\n")

    # ============================================
    # æ­¥éª¤ 2: è´¨é‡æ§åˆ¶
    # ============================================
    filtered_data_path = os.path.join(args.output_dir, "02_filtered_data.h5ad")

    if not ckpt.is_completed('step2_qc'):
        def qc_step():
            adata_filtered, qc_stats = quality_control(
                adata,
                min_genes=args.min_genes,
                max_genes=args.max_genes,
                min_counts=args.min_counts,
                max_pct_mito=args.max_pct_mito,
                doublet_method=args.doublet_method,
                doublet_threshold=args.doublet_threshold,
                output_dir=args.output_dir,
                return_stats=True,
                memory_monitor=mem_monitor
            )

            # ä¿å­˜è´¨æ§åçš„æ•°æ®
            adata_filtered.write(filtered_data_path)

            # ä¿å­˜è´¨æ§ç»Ÿè®¡è¡¨
            qc_stats_path = os.path.join(args.output_dir, "02_qc_statistics.csv")
            qc_stats.to_csv(qc_stats_path, index=False)

            print(f"\nâœ… è´¨æ§å®Œæˆ:")
            print(f"  è¿‡æ»¤åç»†èƒæ•°: {adata_filtered.n_obs:,}")
            print(f"  æ•°æ®å·²ä¿å­˜: {filtered_data_path}")

            # ä¿å­˜æ–­ç‚¹
            ckpt.save_checkpoint('step2_qc',
                               file=filtered_data_path,
                               n_cells_after=adata_filtered.n_obs)

            return adata_filtered

        adata = safe_step_execution(ckpt, 'æ­¥éª¤2: è´¨é‡æ§åˆ¶', qc_step)
    else:
        print("\nâœ“ æ­¥éª¤2å·²å®Œæˆï¼ŒåŠ è½½å·²æœ‰æ•°æ®...")
        adata = sc.read_h5ad(filtered_data_path)
        print(f"  è¿‡æ»¤åç»†èƒæ•°: {adata.n_obs:,}\n")

    # ============================================
    # æ­¥éª¤ 3: æ•°æ®æ•´åˆ
    # ============================================
    integrated_data_path = os.path.join(args.output_dir, "03_integrated_data.h5ad")

    if not ckpt.is_completed('step3_integrate'):
        def integration_step():
            adata_integrated = data_integration(
                adata,
                batch_key="SampleName",
                method=args.integration_method,
                n_pcs=args.n_pcs,
                n_neighbors=args.n_neighbors,
                resolution=args.resolution,
                gene_num=args.gene_num,
                umap_min_dist=args.umap_min_dist,
                output_dir=args.output_dir,
                memory_monitor=mem_monitor
            )

            # ä¿å­˜æ•´åˆåçš„æ•°æ®
            adata_integrated.write(integrated_data_path)

            print(f"\nâœ… æ•°æ®æ•´åˆå®Œæˆ:")
            print(f"  ç»†èƒæ•°: {adata_integrated.n_obs:,}")
            print(f"  èšç±»æ•°: {adata_integrated.obs['leiden'].nunique()}")
            print(f"  æ•°æ®å·²ä¿å­˜: {integrated_data_path}")

            # ä¿å­˜æ–­ç‚¹
            ckpt.save_checkpoint('step3_integrate',
                               file=integrated_data_path,
                               n_clusters=adata_integrated.obs['leiden'].nunique())

            return adata_integrated

        adata = safe_step_execution(ckpt, 'æ­¥éª¤3: æ•°æ®æ•´åˆä¸é™ç»´', integration_step)
    else:
        print("\nâœ“ æ­¥éª¤3å·²å®Œæˆï¼ŒåŠ è½½å·²æœ‰æ•°æ®...")
        adata = sc.read_h5ad(integrated_data_path)
        print(f"  æ•´åˆåç»†èƒæ•°: {adata.n_obs:,}")
        print(f"  èšç±»æ•°: {adata.obs['leiden'].nunique()}\n")

    # ============================================
    # æ­¥éª¤ 4: ç»†èƒç±»å‹æ³¨é‡Š (å¯é€‰)
    # ============================================
    if args.run_annotation:
        annotation_output = os.path.join(args.output_dir, "04_annotated_data.h5ad")

        if not ckpt.is_completed('step4_annotate'):
            def annotation_step():
                adata_annotated = celltypist_annotation(
                    adata,
                    model_path=args.celltypist_model,
                    output_path=annotation_output
                )

                print(f"\nâœ… ç»†èƒç±»å‹æ³¨é‡Šå®Œæˆ:")
                print(f"  æ³¨é‡Šç»“æœå·²ä¿å­˜: {annotation_output}")

                # ä¿å­˜æ–­ç‚¹
                ckpt.save_checkpoint('step4_annotate',
                                   file=annotation_output)

                mem_monitor.checkpoint("ç»†èƒç±»å‹æ³¨é‡Š")

                return adata_annotated

            adata = safe_step_execution(ckpt, 'æ­¥éª¤4: ç»†èƒç±»å‹æ³¨é‡Š', annotation_step)
        else:
            print("\nâœ“ æ­¥éª¤4å·²å®Œæˆï¼Œè·³è¿‡\n")

    # ============================================
    # å®Œæˆæ€»ç»“
    # ============================================
    mem_monitor.checkpoint("æµç¨‹å®Œæˆ")

    # ä¿å­˜å†…å­˜ä½¿ç”¨æŠ¥å‘Š
    mem_summary = mem_monitor.get_summary()
    mem_report_path = os.path.join(args.output_dir, "memory_usage.csv")
    mem_summary.to_csv(mem_report_path, index=False)

    # ç»˜åˆ¶å†…å­˜ä½¿ç”¨å›¾
    mem_plot_path = os.path.join(args.output_dir, "memory_usage.png")
    mem_monitor.plot_memory_usage(mem_plot_path)

    print("\n" + "=" * 100)
    print(" " * 40 + "ğŸ‰ æµç¨‹å®Œæˆï¼")
    print("=" * 100)
    print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {args.output_dir}/")
    print("\nğŸ’¡ æ–­ç‚¹ç»­ä¼ æç¤ºï¼š")
    print("  â€¢ å¦‚é‡é”™è¯¯ï¼Œä¿®å¤åè¿è¡Œ: python sc_pipeline_enhanced.py --resume ...")
    print("  â€¢ æŸ¥çœ‹è¿›åº¦: python sc_pipeline_enhanced.py --show-progress ...")
    print("  â€¢ ä»å¤´å¼€å§‹: python sc_pipeline_enhanced.py --reset ...")
    print("=" * 100 + "\n")


if __name__ == "__main__":
    main()
